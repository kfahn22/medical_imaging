{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "private_outputs": true,
      "authorship_tag": "ABX9TyMThDT1YnTBxcLSxrUG3cbz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kfahn22/medical_imaging/blob/main/Copy_of_image_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using DCGAN to Generate Synthetic Lung Images\n",
        "\n",
        "It is adapted from [here](https://github.com/ovh/ai-training-examples/tree/main/notebooks/computer-vision/image-generation/miniconda/dcgan-image-generation).\n",
        "\n",
        "Apache 2.0 License"
      ],
      "metadata": {
        "id": "8qQCdryE2L8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Instructions for downloading dataset from Kaggle](https://www.kaggle.com/discussions/general/156610)\n",
        "\n",
        "https://www.kaggle.com/discussions/general/371462#2060661"
      ],
      "metadata": {
        "id": "bjbEOpTiEHqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "4NuhVsoBD_y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload() #this will prompt you to upload the kaggle.json"
      ],
      "metadata": {
        "id": "z9EIfrS1EEjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls -lha kaggle.json"
      ],
      "metadata": {
        "id": "ue_n7BB6EQ95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "F2Gq9SovEXwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set permissions"
      ],
      "metadata": {
        "id": "wOHDO5DPE3HO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Move file to folder expected by Kaggle"
      ],
      "metadata": {
        "id": "XBuOrYyfEljV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "iVJDWmX3Ek8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "JXsdInujJoNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "id": "WBUyeMDrEvyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ky0Pv7-5YQ-"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set a seed so generation is reproducable."
      ],
      "metadata": {
        "id": "8G8EZmJY57hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "czhmKw_k5z0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia"
      ],
      "metadata": {
        "id": "ZP1IVS_76lWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip dataset\n",
        "path_to_zip_file = \"chest-xray-pneumonia.zip\"\n",
        "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"chest-xray-pneumonia\")"
      ],
      "metadata": {
        "id": "pXBY9kuJ65hW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/chest-xray-pneumonia /content/gdrive/MyDrive\n",
        "#!cp -r /content/gdrive/MyDrive/ABO_dir /content/BlenderProc/resources"
      ],
      "metadata": {
        "id": "kkGbKKAgKuA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/chest-xray-pneumonia.zip /content/gdrive/MyDrive"
      ],
      "metadata": {
        "id": "n6fuOUxrDHPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "#dataroot = \"chest-xray-pneumonia/chest_xray/train/\"\n",
        "dataroot = \"/content/chest-xray-pneumonia/chest_xray/chest_xray/train\"\n",
        "nb_channels = 3 # For RGB images, but if you use grayscale images, ToTensor() will replicate the single channel into three channels, so you should not have to modify anything\n",
        "image_resize = 64\n",
        "\n",
        "batch_size = 128\n",
        "nb_gpu = 1\n",
        "nb_workers = 2 # based on system resources\n",
        "\n",
        "# GPU or CPU (Not having at least 1 GPU can prevent code from working)\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and nb_gpu > 0) else \"cpu\")"
      ],
      "metadata": {
        "id": "Vw0AjNBC7ETS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataset by applying transformation to our images\n",
        "dataset = dset.ImageFolder(root=dataroot,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(image_resize),\n",
        "                               transforms.CenterCrop(image_resize),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))"
      ],
      "metadata": {
        "id": "IAoSg2EI7IN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of downloaded images: {len(dataset)}')"
      ],
      "metadata": {
        "id": "tNOoQHBF7QOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yunpQV5s7bAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loader"
      ],
      "metadata": {
        "id": "vMD9sJl-7ek9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataloader\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=nb_workers)\n",
        "\n",
        "real_batch = next(iter(dataloader))"
      ],
      "metadata": {
        "id": "bvst1X867dd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_images = 9\n",
        "nb_row = math.ceil(math.sqrt(nb_images))\n",
        "\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0][:nb_images].to(device)[:nb_images], padding=2, normalize=True)))"
      ],
      "metadata": {
        "id": "sQwOPfGO7k5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "TuXuSMVs7rO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of z latent vector (i.e. size of generator input), same size as described in the DCGAN paper\n",
        "nz = 100\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 64\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 64\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 100\n",
        "\n",
        "# Learning rate for optimizers, same value as described in the DCGAN paper\n",
        "lr = 0.0002\n",
        "\n",
        "# Beta1 hyperparameter for Adam optimizers, same value as described in the DCGAN paper\n",
        "beta1 = 0.5"
      ],
      "metadata": {
        "id": "KowgU7dz7qPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "metadata": {
        "id": "rghdnM3k7w0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator Code\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, nb_gpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.nb_gpu = nb_gpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is Z, going into a convolution\n",
        "\n",
        "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. ``(ngf*8) x 4 x 4``\n",
        "\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. ``(ngf*4) x 8 x 8``\n",
        "\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. ``(ngf*2) x 16 x 16``\n",
        "\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # state size. ``(ngf) x 32 x 32``\n",
        "\n",
        "            nn.ConvTranspose2d(ngf, nb_channels, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # state size. ``(nb_channels) x 64 x 64``\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "metadata": {
        "id": "NO4UxycL7_Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the generator\n",
        "netG = Generator(nb_gpu).to(device)\n",
        "\n",
        "# Handle multi-GPU if desired\n",
        "if (device.type == 'cuda') and (nb_gpu > 1):\n",
        "    netG = nn.DataParallel(netG, list(range(nb_gpu)))\n",
        "\n",
        "# Apply the ``weights_init`` funb_channelstion to randomly initialize all weights\n",
        "#  to ``mean=0``, ``stdev=0.02``.\n",
        "netG.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netG)"
      ],
      "metadata": {
        "id": "MxNA72M48Dfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, nb_gpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.nb_gpu = nb_gpu\n",
        "        self.main = nn.Sequential(\n",
        "            # input is ``(nb_channels) x 64 x 64``\n",
        "\n",
        "            nn.Conv2d(nb_channels, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf) x 32 x 32``\n",
        "\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf*2) x 16 x 16``\n",
        "\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf*4) x 8 x 8``\n",
        "\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. ``(ndf*8) x 4 x 4``\n",
        "\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "metadata": {
        "id": "hy_YxMAl8tJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Discriminator\n",
        "netD = Discriminator(nb_gpu).to(device)\n",
        "\n",
        "# Handle multi-GPU if desired\n",
        "if (device.type == 'cuda') and (nb_gpu > 1):\n",
        "    netD = nn.DataParallel(netD, list(range(nb_gpu)))\n",
        "\n",
        "# Apply the ``weights_init`` function to randomly initialize all weights\n",
        "# like this: ``to mean=0, stdev=0.2``.\n",
        "netD.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netD)"
      ],
      "metadata": {
        "id": "g2PM3zh487e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Real image and discriminator thinks it is real - Right decision\n",
        "target = 1\n",
        "output= 0.99\n",
        "\n",
        "print(-(target * math.log(output) + (1-target)*math.log(1-output)))\n",
        "\n",
        "# Real image and discriminator thinks it is fake - False decision\n",
        "target = 1\n",
        "output= 0.01\n",
        "\n",
        "print(-(target * math.log(output) + (1-target)*math.log(1-output)))\n",
        "\n",
        "# False image but discriminator thinks it is real - False decision\n",
        "target = 0\n",
        "output= 0.99\n",
        "\n",
        "print(-(target * math.log(output) + (1-target)*math.log(1-output)))"
      ],
      "metadata": {
        "id": "U3b68QV_9paO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set real and fake label values (following GAN paper convention)\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "_uwCaQlE9t6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup optimizers for both G and D, according to the DCGAN paper\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
      ],
      "metadata": {
        "id": "SjmJae7X9yai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "X2yMWU8R95lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device)"
      ],
      "metadata": {
        "id": "4F77xsKf948l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flags - For each epoch\n",
        "show_images = True\n",
        "save_images = True\n",
        "save_model = True\n",
        "\n",
        "def save_dcgan(netG, netD, path_checkpoint):\n",
        "    checkpoint = {\"g_model_state_dict\": netG.state_dict(),\n",
        "                \"d_model_state_dict\": netD.state_dict(),\n",
        "                }\n",
        "\n",
        "    torch.save(checkpoint, path_checkpoint)\n",
        "\n",
        "def makedir(new_dir):\n",
        "  if not os.path.exists(new_dir):\n",
        "    os.makedirs(new_dir)\n",
        "\n",
        "# Create folders\n",
        "makedir(\"images\")\n",
        "makedir(\"models\")"
      ],
      "metadata": {
        "id": "miJT9GEa98I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "data_len = len(dataloader)\n",
        "\n",
        "# Lists to keep track of progress\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "\n",
        "\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # For each batch in the dataloader (depends on batch_size and your number of images)\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        ## Train with all-real batch\n",
        "        netD.zero_grad()\n",
        "        # Format batch\n",
        "        real_cpu = data[0].to(device)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "        # Forward pass real batch through D\n",
        "        output = netD(real_cpu).view(-1)\n",
        "        # Calculate loss on all-real batch\n",
        "        errD_real = criterion(output, label)\n",
        "        # Calculate gradients for D in backward pass\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        # Generate batch of latent vectors\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "        # Generate fake image batch with G\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label)\n",
        "        # Classify all fake batch with D\n",
        "        output = netD(fake.detach()).view(-1)\n",
        "        # Calculate D's loss on the all-fake batch\n",
        "        errD_fake = criterion(output, label)\n",
        "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        # Compute error of D as sum over the fake and the real batches\n",
        "        errD = errD_real + errD_fake\n",
        "        # Update D\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "        output = netD(fake).view(-1)\n",
        "        # Calculate G's loss based on this output\n",
        "        errG = criterion(output, label)\n",
        "        # Calculate gradients for G\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        # Update G\n",
        "        optimizerG.step()\n",
        "\n",
        "        ############################\n",
        "        # (3) Metrics & Evaluation\n",
        "        ###########################\n",
        "\n",
        "        # Print output training stats every 50 batches (if your dataset is large, printing at every epoch might be less frequent than you want)\n",
        "        if i % 50 == 0:\n",
        "            print(f\"Epoch: {epoch}/{num_epochs} Batches: {i}/{data_len}\\tLoss_D: {errD.item():.4f}   Loss_G: {errG.item():.4f}    D(x): {D_x:.4f}    D(G(z)): {D_G_z1:.4f} / {D_G_z2:.4f}\")\n",
        "\n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "\n",
        "    # Generate fake images to see how the generator is doing by saving G's output on fixed_noise at each epoch (fixed noise allow to obtain similar images).\n",
        "    if show_images == True:\n",
        "        with torch.no_grad():\n",
        "            # Uncomment the line below to generate a new variety of images every time\n",
        "            #fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "            fake = netG(fixed_noise).detach().cpu()\n",
        "            img_list.append(vutils.make_grid(fake[:nb_images], padding=2, normalize=True, nrow=nb_row))\n",
        "\n",
        "            plt.figure(figsize=(3, 3))\n",
        "            plt.axis(\"off\")\n",
        "            plt.imshow(np.transpose(img_list[-1], (1, 2, 0)))\n",
        "\n",
        "            if save_images == True:\n",
        "                plt.savefig(f'images/epoch_{epoch}_gen_images.png')\n",
        "\n",
        "            # Display image\n",
        "            plt.show()\n",
        "\n",
        "    # Save models each 5 epochs\n",
        "    if epoch % 5 == 0:\n",
        "        if save_model:\n",
        "            save_dcgan(netG, netD, path_checkpoint=f\"models/chest_epoch_{epoch}_checkpoint.pkl\")\n",
        "\n",
        "# Save the final models\n",
        "save_dcgan(netG, netD, path_checkpoint=\"models/chest_final_epoch_checkpoint.pkl\")"
      ],
      "metadata": {
        "id": "zBu0JBbT-L05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot losses"
      ],
      "metadata": {
        "id": "JDN4IyXa-UyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vXiXToR5-UAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# increase the default limit set by Matplotlib for embedding animations\n",
        "import matplotlib\n",
        "matplotlib.rcParams['animation.embed_limit'] = 100  # 100 MB"
      ],
      "metadata": {
        "id": "RGMBMnXm-all"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(8, 8))\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=2.0, h_pad=2.0, w_pad=2.0)\n",
        "\n",
        "# Initial empty title and image\n",
        "title = plt.title(\"\")\n",
        "img_plot = plt.imshow(np.transpose(img_list[0], (1, 2, 0)), animated=True)\n",
        "\n",
        "def update_title_and_image(frame):\n",
        "    img_plot.set_array(np.transpose(img_list[frame], (1, 2, 0)))\n",
        "    title.set_text(f\"Epoch {frame}/{num_epochs}\")\n",
        "\n",
        "ani = animation.FuncAnimation(fig, update_title_and_image, frames=len(img_list), interval=100, repeat_delay=5000)\n",
        "\n",
        "# Display animation\n",
        "HTML(ani.to_jshtml())"
      ],
      "metadata": {
        "id": "9gf7YEvx-ddA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the animation as a GIF\n",
        "ani.save('images/dcgan_training_animation.gif', writer='pillow')"
      ],
      "metadata": {
        "id": "I6QeU2HS-j9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "6pQtU6k6_AUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify desired weights path\n",
        "path_checkpoint = \"models/chest_final_epoch_checkpoint.pkl\""
      ],
      "metadata": {
        "id": "9cYpWYng-2Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of desired images\n",
        "num_img = 16\n",
        "nb_row = math.ceil(math.sqrt(num_img))\n",
        "\n",
        "# Create a random noise\n",
        "random_noise = torch.randn(num_img, nz, 1, 1, device=device)\n",
        "\n",
        "# Instantiate a generator\n",
        "new_gen= Generator(nb_gpu).to(device)\n",
        "\n",
        "# Handle multi-GPU if desired\n",
        "if (device.type == 'cuda') and (nb_gpu > 1):\n",
        "    new_gen = nn.DataParallel(new_gen, list(range(nb_gpu)))\n",
        "\n",
        "# Load weights from path\n",
        "checkpoint = torch.load(path_checkpoint, map_location=\"cpu\")\n",
        "state_dict_g = checkpoint[\"g_model_state_dict\"]\n",
        "new_gen.load_state_dict(state_dict_g)\n",
        "\n",
        "# Generate images\n",
        "with torch.no_grad():\n",
        "    fake_data = new_gen(random_noise).detach().cpu()\n",
        "\n",
        "# Plot images\n",
        "img_grid = vutils.make_grid(fake_data, padding=2, normalize=True, nrow=nb_row).cpu()\n",
        "img_grid = np.transpose(img_grid, (1, 2, 0))\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(img_grid)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7ez6kBwh-3q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Follow the training"
      ],
      "metadata": {
        "id": "EwQ4gmf6B52e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# increase the default limit set by Matplotlib for embedding animations\n",
        "import matplotlib\n",
        "matplotlib.rcParams['animation.embed_limit'] = 100  # 100 MB"
      ],
      "metadata": {
        "id": "v5qx3EqdB7nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(8, 8))\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=2.0, h_pad=2.0, w_pad=2.0)\n",
        "\n",
        "# Initial empty title and image\n",
        "title = plt.title(\"\")\n",
        "img_plot = plt.imshow(np.transpose(img_list[0], (1, 2, 0)), animated=True)\n",
        "\n",
        "def update_title_and_image(frame):\n",
        "    img_plot.set_array(np.transpose(img_list[frame], (1, 2, 0)))\n",
        "    title.set_text(f\"Epoch {frame}/{num_epochs}\")\n",
        "\n",
        "ani = animation.FuncAnimation(fig, update_title_and_image, frames=len(img_list), interval=100, repeat_delay=5000)\n",
        "\n",
        "# Display animation\n",
        "HTML(ani.to_jshtml())"
      ],
      "metadata": {
        "id": "pqVseMdtCGTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the animation as a GIF\n",
        "ani.save('images/dcgan_training_animation.gif', writer='pillow')"
      ],
      "metadata": {
        "id": "rrNd_theCLii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ani.save('gdrive/MyDrive/dcgan_training_animation.gif',  writer='pillow')"
      ],
      "metadata": {
        "id": "pNHJj20hnviW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "zSdFwjz4CQJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify desired weights path\n",
        "path_checkpoint = \"models/chest_final_epoch_checkpoint.pkl\""
      ],
      "metadata": {
        "id": "GUM1S3HACPEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of desired images\n",
        "num_img = 16\n",
        "nb_row = math.ceil(math.sqrt(num_img))\n",
        "\n",
        "# Create a random noise\n",
        "random_noise = torch.randn(num_img, nz, 1, 1, device=device)\n",
        "\n",
        "# Instantiate a generator\n",
        "new_gen= Generator(nb_gpu).to(device)\n",
        "\n",
        "# Handle multi-GPU if desired\n",
        "if (device.type == 'cuda') and (nb_gpu > 1):\n",
        "    new_gen = nn.DataParallel(new_gen, list(range(nb_gpu)))\n",
        "\n",
        "# Load weights from path\n",
        "checkpoint = torch.load(path_checkpoint, map_location=\"cpu\")\n",
        "state_dict_g = checkpoint[\"g_model_state_dict\"]\n",
        "new_gen.load_state_dict(state_dict_g)\n",
        "\n",
        "# Generate images\n",
        "with torch.no_grad():\n",
        "    fake_data = new_gen(random_noise).detach().cpu()\n",
        "\n",
        "# Plot images\n",
        "img_grid = vutils.make_grid(fake_data, padding=2, normalize=True, nrow=nb_row).cpu()\n",
        "img_grid = np.transpose(img_grid, (1, 2, 0))\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(img_grid)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MXCvcQEgCVZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post Processing"
      ],
      "metadata": {
        "id": "5wMwqL-kCeeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a discriminator\n",
        "new_dis= Discriminator(nb_gpu).to(device)\n",
        "\n",
        "# Handle multi-GPU if desired\n",
        "if (device.type == 'cuda') and (nb_gpu > 1):\n",
        "    new_dis = nn.DataParallel(new_dis, list(range(nb_gpu)))\n",
        "\n",
        "# Load weights from path\n",
        "checkpoint = torch.load(path_checkpoint, map_location=\"cpu\")\n",
        "state_dict_d = checkpoint[\"d_model_state_dict\"]\n",
        "new_dis.load_state_dict(state_dict_d)"
      ],
      "metadata": {
        "id": "RoGkh4VFCdxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set threshold\n",
        "threshold = 0.70\n",
        "\n",
        "# Create list to store the images classified as >= threshold\n",
        "verified_images = []\n",
        "\n",
        "# Till we do not have 64 images\n",
        "\n",
        "while len(verified_images)<64:\n",
        "    # Create a random noise\n",
        "    random_noise = torch.randn(num_img, nz, 1, 1, device=device)\n",
        "    # Generate images for this noise\n",
        "    with torch.no_grad():\n",
        "        fake_data = new_gen(random_noise)\n",
        "\n",
        "        # Set the discriminator to eval mode\n",
        "        new_dis.eval()\n",
        "\n",
        "        # Pass the generated fake_data through the discriminator & obtain real/fake probabilities\n",
        "        output_probabilities = new_dis(fake_data)\n",
        "\n",
        "    # Create mask (If prob < threshold, set to False, else True)\n",
        "    good_image_mask = output_probabilities >= threshold\n",
        "\n",
        "    # Remove extra tensor dims (torch.Size([16, 1, 1, 1]) -> torch.Size([16]))\n",
        "    good_image_mask = good_image_mask.squeeze()\n",
        "\n",
        "    # Keep the selected images\n",
        "    good_images = fake_data[good_image_mask > threshold]\n",
        "\n",
        "    # If good_images is not None\n",
        "    if good_images.numel() > 0:\n",
        "        # Loop through the tensor along the first dimension (index 0), since we have x images in a torch.Size([x, 3, 64, 64]) object\n",
        "        for i in range(good_images.size(0)):\n",
        "            # Add selected images\n",
        "            verified_images.append(good_images[i])\n",
        "\n",
        "# Plot selected images\n",
        "img_grid = vutils.make_grid(verified_images, padding=2, normalize=True, nrow=8).cpu()\n",
        "img_grid = np.transpose(img_grid, (1, 2, 0))\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(img_grid)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "khu2bxfaCkr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Curated Images"
      ],
      "metadata": {
        "id": "01sbeNIICsAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_images =[]"
      ],
      "metadata": {
        "id": "mNO2kXR6nfrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_keep=[13]\n",
        "\n",
        "for i in index_to_keep:\n",
        "    filtered_images.append(verified_images[i])\n",
        "\n",
        "print(f'Number of curated images {len(filtered_images)}')"
      ],
      "metadata": {
        "id": "XPFu1oN3CuiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot real training images and our curated images\n",
        "nb_row = math.ceil(math.sqrt(len(filtered_images)))\n",
        "\n",
        "# Real images\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.subplot(1,2,1)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Real Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:len(filtered_images)], padding=2, normalize=True, nrow=nb_row).cpu(),(1,2,0)))\n",
        "\n",
        "# Fake images\n",
        "img_grid = vutils.make_grid(filtered_images, padding=2, normalize=True, nrow=nb_row).cpu()\n",
        "img_grid = np.transpose(img_grid, (1, 2, 0))\n",
        "plt.subplot(1,2,2)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Curated images\")\n",
        "plt.imshow(img_grid)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cpRRVjLDCySP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}